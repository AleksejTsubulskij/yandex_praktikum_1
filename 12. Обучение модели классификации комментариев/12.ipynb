{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /opt/conda/lib/python3.9/site-packages (3.3.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from lightgbm) (1.9.1)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.9/site-packages (from lightgbm) (0.36.2)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /opt/conda/lib/python3.9/site-packages (from lightgbm) (0.24.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from lightgbm) (1.21.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn!=0.22.0->lightgbm) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jovyan/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/conda/lib/python3.9/site-packages (3.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (1.21.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from spacy) (49.6.0.post20210108)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /opt/conda/lib/python3.9/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /opt/conda/lib/python3.9/site-packages (from spacy) (8.0.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (0.7.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /opt/conda/lib/python3.9/site-packages (from spacy) (2.4.4)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.9/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (1.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.9/site-packages (from spacy) (0.6.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from spacy) (3.0.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.9/site-packages (from spacy) (3.0.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/conda/lib/python3.9/site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/conda/lib/python3.9/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (4.61.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /opt/conda/lib/python3.9/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (4.3.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.6)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from jinja2->spacy) (2.1.1)\n",
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "\u001b[K     |██████████████████████▉         | 9.9 MB 7.2 kB/s eta 0:09:17\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 519, in read\n",
      "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 62, in read\n",
      "    data = self.__fp.read(amt)\n",
      "  File \"/opt/conda/lib/python3.9/http/client.py\", line 455, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"/opt/conda/lib/python3.9/http/client.py\", line 499, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"/opt/conda/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/opt/conda/lib/python3.9/ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/opt/conda/lib/python3.9/ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/cli/base_command.py\", line 180, in _main\n",
      "    status = self.run(options, args)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/cli/req_command.py\", line 205, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/commands/install.py\", line 318, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 103, in resolve\n",
      "    r = self.factory.make_requirement_from_install_req(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/factory.py\", line 429, in make_requirement_from_install_req\n",
      "    cand = self._make_candidate_from_link(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/factory.py\", line 200, in _make_candidate_from_link\n",
      "    self._link_candidate_cache[link] = LinkCandidate(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 306, in __init__\n",
      "    super().__init__(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 151, in __init__\n",
      "    self.dist = self._prepare()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 234, in _prepare\n",
      "    dist = self._prepare_distribution()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 317, in _prepare_distribution\n",
      "    return self._factory.preparer.prepare_linked_requirement(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/operations/prepare.py\", line 508, in prepare_linked_requirement\n",
      "    return self._prepare_linked_requirement(req, parallel_builds)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/operations/prepare.py\", line 550, in _prepare_linked_requirement\n",
      "    local_file = unpack_url(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/operations/prepare.py\", line 239, in unpack_url\n",
      "    file = get_http_url(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/operations/prepare.py\", line 102, in get_http_url\n",
      "    from_path, content_type = download(link, temp_dir.path)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/network/download.py\", line 157, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/cli/progress_bars.py\", line 152, in iter\n",
      "    for x in it:\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_internal/network/utils.py\", line 62, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 576, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 541, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"/opt/conda/lib/python3.9/contextlib.py\", line 135, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 443, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='objects.githubusercontent.com', port=443): Read timed out.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.simplefilter('ignore')\n",
    "from scipy import stats as st\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'svg' \n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 8, 5\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "     GridSearchCV, \n",
    "    RandomizedSearchCV,\n",
    "    train_test_split\n",
    ")\n",
    "import lightgbm\n",
    "import re\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "from tqdm import notebook\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "from tqdm import notebook\n",
    "from sklearn.metrics import f1_score\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install spacy\n",
    "# # Download spaCy's  'en' Model\n",
    "# !{sys.executable} -m spacy download en\n",
    "# import spacy\n",
    "# # Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "# nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0\n",
       "5  \"\\n\\nCongratulations from me as well, use the ...      0\n",
       "6       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1\n",
       "7  Your vandalism to the Matt Shirvington article...      0\n",
       "8  Sorry if the word 'nonsense' was offensive to ...      0\n",
       "9  alignment on this subject and which are contra...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try :\n",
    "    data= pd.read_csv('/datasets/toxic_comments.csv', index_col=[0])\n",
    "    \n",
    "except:\n",
    "    data  = pd.read_csv('toxic_comments.csv', index_col=[0])\n",
    "\n",
    "display(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143106\n",
       "1     16186\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data['toxic'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\n",
       "<svg height=\"286.2pt\" version=\"1.1\" viewBox=\"0 0 299.878125 286.2\" width=\"299.878125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2023-12-06T14:49:22.113092</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 286.2 \n",
       "L 299.878125 286.2 \n",
       "L 299.878125 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill:none;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 265.498125 143.1 \n",
       "C 265.498125 130.281609 263.231139 117.563646 258.80209 105.534736 \n",
       "C 254.37304 93.505827 247.851328 82.354456 239.538798 72.596742 \n",
       "C 231.226267 62.839028 221.25317 54.627868 210.081091 48.3433 \n",
       "C 198.909012 42.058732 186.713011 37.79923 174.057554 35.761937 \n",
       "C 161.402097 33.724644 148.485488 33.941484 135.905546 36.402419 \n",
       "C 123.325604 38.863354 111.279448 43.529824 100.324611 50.185851 \n",
       "C 89.369774 56.841877 79.677911 65.383165 71.697589 75.414397 \n",
       "C 63.717268 85.445629 57.573536 96.809623 53.550742 108.980419 \n",
       "C 49.527948 121.151215 47.689128 133.938105 48.119388 146.749273 \n",
       "C 48.549647 159.560441 51.242244 172.195144 56.072558 184.068611 \n",
       "C 60.902871 195.942078 67.795213 206.868259 76.430585 216.341458 \n",
       "C 85.065957 225.814657 95.309048 233.686435 106.685778 239.592462 \n",
       "C 118.062509 245.498489 130.394611 249.346222 143.11132 250.957577 \n",
       "C 155.828029 252.568931 168.730081 251.918658 181.220331 249.036854 \n",
       "C 193.710582 246.155049 205.593315 241.08687 216.318565 234.066886 \n",
       "C 227.043814 227.046903 236.443521 218.185113 244.082639 207.891667 \n",
       "L 156.778125 143.1 \n",
       "L 265.498125 143.1 \n",
       "z\n",
       "\" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 244.082639 207.891667 \n",
       "C 250.991593 198.582093 256.36722 188.22707 260.005506 177.219586 \n",
       "C 263.643793 166.212102 265.498124 154.693191 265.498125 143.10001 \n",
       "L 156.778125 143.1 \n",
       "L 244.082639 207.891667 \n",
       "z\n",
       "\" style=\"fill:#ff7f0e;\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\"/>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"text_1\">\n",
       "     <!-- toxic -->\n",
       "     <g transform=\"translate(14.798438 155.060937)rotate(-90)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path d=\"M 18.3125 70.21875 \n",
       "L 18.3125 54.6875 \n",
       "L 36.8125 54.6875 \n",
       "L 36.8125 47.703125 \n",
       "L 18.3125 47.703125 \n",
       "L 18.3125 18.015625 \n",
       "Q 18.3125 11.328125 20.140625 9.421875 \n",
       "Q 21.96875 7.515625 27.59375 7.515625 \n",
       "L 36.8125 7.515625 \n",
       "L 36.8125 0 \n",
       "L 27.59375 0 \n",
       "Q 17.1875 0 13.234375 3.875 \n",
       "Q 9.28125 7.765625 9.28125 18.015625 \n",
       "L 9.28125 47.703125 \n",
       "L 2.6875 47.703125 \n",
       "L 2.6875 54.6875 \n",
       "L 9.28125 54.6875 \n",
       "L 9.28125 70.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-116\"/>\n",
       "       <path d=\"M 30.609375 48.390625 \n",
       "Q 23.390625 48.390625 19.1875 42.75 \n",
       "Q 14.984375 37.109375 14.984375 27.296875 \n",
       "Q 14.984375 17.484375 19.15625 11.84375 \n",
       "Q 23.34375 6.203125 30.609375 6.203125 \n",
       "Q 37.796875 6.203125 41.984375 11.859375 \n",
       "Q 46.1875 17.53125 46.1875 27.296875 \n",
       "Q 46.1875 37.015625 41.984375 42.703125 \n",
       "Q 37.796875 48.390625 30.609375 48.390625 \n",
       "z\n",
       "M 30.609375 56 \n",
       "Q 42.328125 56 49.015625 48.375 \n",
       "Q 55.71875 40.765625 55.71875 27.296875 \n",
       "Q 55.71875 13.875 49.015625 6.21875 \n",
       "Q 42.328125 -1.421875 30.609375 -1.421875 \n",
       "Q 18.84375 -1.421875 12.171875 6.21875 \n",
       "Q 5.515625 13.875 5.515625 27.296875 \n",
       "Q 5.515625 40.765625 12.171875 48.375 \n",
       "Q 18.84375 56 30.609375 56 \n",
       "z\n",
       "\" id=\"DejaVuSans-111\"/>\n",
       "       <path d=\"M 54.890625 54.6875 \n",
       "L 35.109375 28.078125 \n",
       "L 55.90625 0 \n",
       "L 45.3125 0 \n",
       "L 29.390625 21.484375 \n",
       "L 13.484375 0 \n",
       "L 2.875 0 \n",
       "L 24.125 28.609375 \n",
       "L 4.6875 54.6875 \n",
       "L 15.28125 54.6875 \n",
       "L 29.78125 35.203125 \n",
       "L 44.28125 54.6875 \n",
       "z\n",
       "\" id=\"DejaVuSans-120\"/>\n",
       "       <path d=\"M 9.421875 54.6875 \n",
       "L 18.40625 54.6875 \n",
       "L 18.40625 0 \n",
       "L 9.421875 0 \n",
       "z\n",
       "M 9.421875 75.984375 \n",
       "L 18.40625 75.984375 \n",
       "L 18.40625 64.59375 \n",
       "L 9.421875 64.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-105\"/>\n",
       "       <path d=\"M 48.78125 52.59375 \n",
       "L 48.78125 44.1875 \n",
       "Q 44.96875 46.296875 41.140625 47.34375 \n",
       "Q 37.3125 48.390625 33.40625 48.390625 \n",
       "Q 24.65625 48.390625 19.8125 42.84375 \n",
       "Q 14.984375 37.3125 14.984375 27.296875 \n",
       "Q 14.984375 17.28125 19.8125 11.734375 \n",
       "Q 24.65625 6.203125 33.40625 6.203125 \n",
       "Q 37.3125 6.203125 41.140625 7.25 \n",
       "Q 44.96875 8.296875 48.78125 10.40625 \n",
       "L 48.78125 2.09375 \n",
       "Q 45.015625 0.34375 40.984375 -0.53125 \n",
       "Q 36.96875 -1.421875 32.421875 -1.421875 \n",
       "Q 20.0625 -1.421875 12.78125 6.34375 \n",
       "Q 5.515625 14.109375 5.515625 27.296875 \n",
       "Q 5.515625 40.671875 12.859375 48.328125 \n",
       "Q 20.21875 56 33.015625 56 \n",
       "Q 37.15625 56 41.109375 55.140625 \n",
       "Q 45.0625 54.296875 48.78125 52.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-99\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-116\"/>\n",
       "      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "      <use x=\"97.265625\" xlink:href=\"#DejaVuSans-120\"/>\n",
       "      <use x=\"156.445312\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "      <use x=\"184.228516\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_2\">\n",
       "    <!-- 0 -->\n",
       "    <g transform=\"translate(36.865504 108.327836)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path d=\"M 31.78125 66.40625 \n",
       "Q 24.171875 66.40625 20.328125 58.90625 \n",
       "Q 16.5 51.421875 16.5 36.375 \n",
       "Q 16.5 21.390625 20.328125 13.890625 \n",
       "Q 24.171875 6.390625 31.78125 6.390625 \n",
       "Q 39.453125 6.390625 43.28125 13.890625 \n",
       "Q 47.125 21.390625 47.125 36.375 \n",
       "Q 47.125 51.421875 43.28125 58.90625 \n",
       "Q 39.453125 66.40625 31.78125 66.40625 \n",
       "z\n",
       "M 31.78125 74.21875 \n",
       "Q 44.046875 74.21875 50.515625 64.515625 \n",
       "Q 56.984375 54.828125 56.984375 36.375 \n",
       "Q 56.984375 17.96875 50.515625 8.265625 \n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \n",
       "Q 6.59375 17.96875 6.59375 36.375 \n",
       "Q 6.59375 54.828125 13.0625 64.515625 \n",
       "Q 19.53125 74.21875 31.78125 74.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-48\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_3\">\n",
       "    <!-- 1 -->\n",
       "    <g transform=\"translate(270.328245 183.390919)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path d=\"M 12.40625 8.296875 \n",
       "L 28.515625 8.296875 \n",
       "L 28.515625 63.921875 \n",
       "L 10.984375 60.40625 \n",
       "L 10.984375 69.390625 \n",
       "L 28.421875 72.90625 \n",
       "L 38.28125 72.90625 \n",
       "L 38.28125 8.296875 \n",
       "L 54.390625 8.296875 \n",
       "L 54.390625 0 \n",
       "L 12.40625 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-49\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['toxic'].value_counts().plot(\n",
    "    kind='pie', \n",
    "    figsize=(10, 5), \n",
    "    legend=False, \n",
    "   \n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "видим что существует дисбаланс классов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize with POS Tag\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = text.lower()\n",
    "    lemm_text = \" \".join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(text)])\n",
    "    cleared_text = re.sub(r'[^a-zA-Z]', ' ', lemm_text) \n",
    "    return \" \".join(cleared_text.split())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lemm_text'] = data['text'][:5].apply(lemmatize_text)\n",
    "\n",
    "data = data.drop(['text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits make under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>d aww he match this background colour i m seem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>hey man i m really not try to edit war it s ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>more i ca n t make any real suggestion on impr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>you sir be my hero any chance you remember wha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   toxic                                          lemm_text\n",
       "0      0  explanation why the edits make under my userna...\n",
       "1      0  d aww he match this background colour i m seem...\n",
       "2      0  hey man i m really not try to edit war it s ju...\n",
       "3      0  more i ca n t make any real suggestion on impr...\n",
       "4      0  you sir be my hero any chance you remember wha..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data['toxic']\n",
    "features = data.drop(['toxic'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, target_train, target_test = train_test_split(features, \n",
    "                                                                              target, \n",
    "                                                                              test_size=0.2, \n",
    "                                                                              random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)\n",
    "features_train = count_tf_idf.fit_transform(features_train['lemm_text'].values)\n",
    "\n",
    "features_test = count_tf_idf.transform(features_test['lemm_text'].values)\n",
    "print(features_train.shape)\n",
    "\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_lr = LogisticRegression()\n",
    "model_LGBM= LGBMClassifier(class_weight='balanced') \n",
    "n_iter_search = 12 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные были загружены , разделены на обучающую и тестовую выборку , также был выявлен дисбаланс класссов ,была проведена лемметизация текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=1234, class_weight='balanced')\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'max_iter': [10,20,40,60, 100],\n",
    "}\n",
    "\n",
    "search_lr = GridSearchCV(model, param_grid, cv=5, verbose=1, scoring='f1')\n",
    "search_lr.fit(features_train, target_train)\n",
    "\n",
    "print(search_lr.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(search_lr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classificator = LogisticRegression(random_state=1234,C=10,max_iter=100)\n",
    "train_f1 = cross_val_score(classificator, \n",
    "                      features_train, \n",
    "                      target_train, \n",
    "                      cv=3, \n",
    "                      scoring='f1').mean()\n",
    "print('F1 на CV', train_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classificator = LogisticRegression(random_state=1234,C=10,max_iter=100,class_weight='balanced')\n",
    "train_f1 = cross_val_score(classificator, \n",
    "                      features_train, \n",
    "                      target_train, \n",
    "                      cv=3, \n",
    "                      scoring='f1').mean()\n",
    "print('F1 на CV', train_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим что логистическая регрессия с сбалансированными весами классов показывает себя лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "parameters_LGBM={'num_leaves':[31, 100, 200], \n",
    "                'learning_rate':[0.1, 0.3, 0.5,0.6,0.8,1],\n",
    "                'random_state':[12345]\n",
    "                }\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(model_LGBM, \n",
    "                                   param_distributions=parameters_LGBM, \n",
    "                                   n_iter=5, \n",
    "                                   cv=31, \n",
    "                                   scoring = 'f1',\n",
    "                                   \n",
    "                                      ) \n",
    "random_search.fit(features_train, target_train) \n",
    "LGBM_final =random_search.best_score_\n",
    "print(\"Лучшие параметры: {}\".format(random_search.best_params_)) \n",
    "print(\"Лучшая оценка : {}\".format(random_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Лучший результат в модели LGBM',random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LGBM= LGBMClassifier(random_state= 12345,num_leaves=200,learning_rate=0.1)\n",
    "\n",
    "train_f1 = cross_val_score(model_LGBM, \n",
    "                      features_train, \n",
    "                      target_train, \n",
    "                      cv=3, \n",
    "                      scoring='f1').mean()\n",
    "print('F1 на CV', train_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = ['LinearRegression',\n",
    "         \n",
    "         \n",
    "        \n",
    "         \n",
    "         'LGBMRegressor',\n",
    "         \n",
    "        ]\n",
    "d = {'f1 модели ':[ 0.769,\n",
    "                                          0.765,\n",
    "                                                \n",
    "                                              ]\n",
    "   }\n",
    "scores_data = pd.DataFrame(data=d, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(scores_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LGBMRegressor показал чуть худшие резульатты при более значительных вычислительных затратах чем линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим результаты модели на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificator.fit(features_train, target_train)\n",
    "target_predict = classificator.predict(features_test)\n",
    "test_f1_LR = f1_score(target_test, target_predict)\n",
    "\n",
    "print('F1 на тестировании', test_f1_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"vect\", TfidfVectorizer()),\n",
    "        (\"logreg\", LogisticRegression(random_state=1234, class_weight='balanced')),\n",
    "    ]\n",
    ")\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'max_iter': [10,20,40,60, 100],\n",
    "}\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=parameter_grid,\n",
    "    n_iter=40,\n",
    "    random_state=1234,\n",
    "    n_jobs=2,\n",
    "    verbose=1,\n",
    ")\n",
    "random_search.fit(features, target) \n",
    "\n",
    "print(\"Лучшие параметры: {}\".format(random_search.best_params_)) \n",
    "print(\"Лучшая оценка : {}\".format(random_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод\n",
    "В ходе работы было сделано:\n",
    "\n",
    "-Подготовленны данные обучения.\n",
    "\n",
    "-Установлен дисбаланс классов и выбран способ решения проблемы, сформированы обучающая, и тестовая выборка.\n",
    " -Обучены модели и выбрана лучшая из них \n",
    "\n",
    "-Показаны параметры качества моделей.\n",
    "\n",
    "-Далее лучшая модель LogisticRegression протестирована на тестовой выборке с F1 на тестировании 0.77\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чек-лист проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [ ]  Весь код выполняется без ошибок\n",
    "- [ ]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [ ]  Данные загружены и подготовлены\n",
    "- [ ]  Модели обучены\n",
    "- [ ]  Значение метрики *F1* не меньше 0.75\n",
    "- [ ]  Выводы написаны"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 2660,
    "start_time": "2023-12-05T15:50:30.522Z"
   },
   {
    "duration": 14976,
    "start_time": "2023-12-05T15:50:33.184Z"
   },
   {
    "duration": 781,
    "start_time": "2023-12-05T15:50:48.162Z"
   },
   {
    "duration": 6,
    "start_time": "2023-12-05T15:50:48.945Z"
   },
   {
    "duration": 83,
    "start_time": "2023-12-05T15:50:48.952Z"
   },
   {
    "duration": 4,
    "start_time": "2023-12-05T15:50:49.037Z"
   },
   {
    "duration": 1138896,
    "start_time": "2023-12-05T15:50:49.042Z"
   },
   {
    "duration": 1560,
    "start_time": "2023-12-05T16:09:47.939Z"
   },
   {
    "duration": 1,
    "start_time": "2023-12-05T16:09:49.501Z"
   },
   {
    "duration": 0,
    "start_time": "2023-12-05T16:09:49.503Z"
   },
   {
    "duration": 0,
    "start_time": "2023-12-05T16:09:49.504Z"
   },
   {
    "duration": 0,
    "start_time": "2023-12-05T16:09:49.505Z"
   },
   {
    "duration": 0,
    "start_time": "2023-12-05T16:09:49.506Z"
   },
   {
    "duration": 0,
    "start_time": "2023-12-05T16:09:49.507Z"
   },
   {
    "duration": 0,
    "start_time": "2023-12-05T16:09:49.508Z"
   },
   {
    "duration": 0,
    "start_time": "2023-12-05T16:09:49.508Z"
   },
   {
    "duration": 0,
    "start_time": "2023-12-05T16:09:49.509Z"
   },
   {
    "duration": 0,
    "start_time": "2023-12-05T16:09:49.510Z"
   },
   {
    "duration": 0,
    "start_time": "2023-12-05T16:09:49.511Z"
   },
   {
    "duration": 0,
    "start_time": "2023-12-05T16:09:49.512Z"
   },
   {
    "duration": 0,
    "start_time": "2023-12-05T16:09:49.513Z"
   },
   {
    "duration": 1,
    "start_time": "2023-12-05T16:09:49.513Z"
   },
   {
    "duration": 0,
    "start_time": "2023-12-05T16:09:49.514Z"
   },
   {
    "duration": 1,
    "start_time": "2023-12-05T16:09:49.515Z"
   },
   {
    "duration": 5353,
    "start_time": "2023-12-06T06:38:27.485Z"
   },
   {
    "duration": 19778,
    "start_time": "2023-12-06T06:38:32.840Z"
   },
   {
    "duration": 2216,
    "start_time": "2023-12-06T06:38:52.620Z"
   },
   {
    "duration": 6,
    "start_time": "2023-12-06T06:38:54.838Z"
   },
   {
    "duration": 85,
    "start_time": "2023-12-06T06:38:54.845Z"
   },
   {
    "duration": 3,
    "start_time": "2023-12-06T06:38:54.932Z"
   },
   {
    "duration": 1525,
    "start_time": "2023-12-06T06:38:54.937Z"
   },
   {
    "duration": 32,
    "start_time": "2023-12-06T06:39:40.018Z"
   },
   {
    "duration": 2471,
    "start_time": "2023-12-06T06:46:59.103Z"
   },
   {
    "duration": 17606,
    "start_time": "2023-12-06T06:47:01.576Z"
   },
   {
    "duration": 838,
    "start_time": "2023-12-06T06:47:19.183Z"
   },
   {
    "duration": 6,
    "start_time": "2023-12-06T06:47:20.023Z"
   },
   {
    "duration": 84,
    "start_time": "2023-12-06T06:47:20.030Z"
   },
   {
    "duration": 313,
    "start_time": "2023-12-06T06:47:35.972Z"
   },
   {
    "duration": 16,
    "start_time": "2023-12-06T06:48:04.354Z"
   },
   {
    "duration": 3,
    "start_time": "2023-12-06T06:48:39.680Z"
   },
   {
    "duration": 375,
    "start_time": "2023-12-06T06:48:48.424Z"
   },
   {
    "duration": 3,
    "start_time": "2023-12-06T06:49:26.174Z"
   },
   {
    "duration": 364,
    "start_time": "2023-12-06T06:49:29.278Z"
   },
   {
    "duration": 3002,
    "start_time": "2023-12-06T14:38:12.291Z"
   },
   {
    "duration": 664211,
    "start_time": "2023-12-06T14:38:15.296Z"
   },
   {
    "duration": 2350,
    "start_time": "2023-12-06T14:49:19.509Z"
   },
   {
    "duration": 6,
    "start_time": "2023-12-06T14:49:21.861Z"
   },
   {
    "duration": 253,
    "start_time": "2023-12-06T14:49:21.869Z"
   },
   {
    "duration": 5,
    "start_time": "2023-12-06T14:49:22.123Z"
   },
   {
    "duration": 5,
    "start_time": "2023-12-06T14:49:22.129Z"
   },
   {
    "duration": 1497,
    "start_time": "2023-12-06T14:49:22.135Z"
   },
   {
    "duration": 8,
    "start_time": "2023-12-06T14:49:23.634Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
